{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import git\n",
    "\n",
    "repo = git.Repo(Path(\".\").absolute(), search_parent_directories=True)\n",
    "ROOT = Path(repo.working_tree_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from tqdm.auto import trange\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pos_bigram_ixs(ix_sents, device=None):\n",
    "    batch_size = ix_sents.shape[0]\n",
    "    sent_lengths = ix_sents.sign().sum(dim=1)\n",
    "    ixs = (\n",
    "        (torch.rand((batch_size,), device=device) * (sent_lengths.float() - 1))\n",
    "        .floor()\n",
    "        .long()\n",
    "        .view(-1, 1)\n",
    "    )\n",
    "    return ix_sents[\n",
    "        torch.arange(batch_size).view(-1, 1), torch.cat((ixs, (ixs + 1)), dim=1),\n",
    "    ]\n",
    "\n",
    "\n",
    "def gen_neg_bigram_ixs(ix_sents, device=None):\n",
    "    batch_size, chunk_size = ix_sents.shape\n",
    "    sent_lengths = ix_sents.sign().sum(dim=1)\n",
    "    # ``distr_ixs1`` determines from where to sample the first index.\n",
    "    # All indices but the last one allows for combining with ``sent_lengths`` - 1\n",
    "    # different second indices.\n",
    "    distr_ixs1 = (sent_lengths.view(-1, 1) - 1) * ix_sents.sign().to(device)\n",
    "    # The last index allows for combining with ``sent_lengths`` different second indices\n",
    "    distr_ixs1[torch.arange(batch_size), sent_lengths - 1] = sent_lengths\n",
    "    ixs1 = Categorical(distr_ixs1.float()).sample().view(-1, 1)\n",
    "    # ``distr_ixs2`` determines from where to sample the second index.\n",
    "    # The boundary case is resolved by initializing  ``distr_ixs2`` with\n",
    "    # one extra column and then removing it later.\n",
    "    distr_ixs2 = torch.zeros((batch_size, chunk_size + 1), device=device)\n",
    "    distr_ixs2[:, :-1] = ix_sents.sign()\n",
    "    # The indices that lead to positive bigrams are avoided by setting\n",
    "    # the corresponding value in ``distribution`` to zero.\n",
    "    distr_ixs2[torch.arange(batch_size).view(-1, 1), ixs1 + 1] = 0\n",
    "    distr_ixs2 = distr_ixs2[:, :-1]\n",
    "    ixs2 = Categorical(distr_ixs2).sample().view(-1, 1)\n",
    "    return ix_sents[\n",
    "        torch.arange(batch_size).view(-1, 1), torch.cat((ixs1, ixs2), dim=1)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from src.misc import WV, BigramEncoder, load_wiki, process_word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_TEXT = ROOT / \"data/raw/crawl-300d-2M.vec\"\n",
    "word2index, word_vecs = process_word_vecs(FAST_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = load_wiki(max_len=25)\n",
    "# Note that the word embeddings are normalize.\n",
    "wv = WV(F.normalize(word_vecs), word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_sents, sent_lengths = wv.to_ix_sents(\n",
    "    sentences, return_sent_lengths=True, adjust=True\n",
    ")\n",
    "perm = torch.randperm(len(ix_sents))\n",
    "ix_sents = ix_sents[perm]\n",
    "sent_lengths = sent_lengths[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Bigram Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bigram_encoder(\n",
    "    bigram_encoder_name, wv, ix_sents, batch_size, smoke_test=False, device=None\n",
    "):\n",
    "    ix_sents = ix_sents.to(device)\n",
    "    if smoke_test:\n",
    "        ix_sents = ix_sents[:1000]\n",
    "    bigram_encoder = BigramEncoder(bigram_encoder_name)\n",
    "    cos = nn.CosineSimilarity(dim=2)\n",
    "    result_comparison = torch.tensor([], device=device).bool()\n",
    "    result_pos_dist = torch.tensor([], device=device).float()\n",
    "    result_neg_dist = torch.tensor([], device=device).float()\n",
    "    for i in trange(0, len(ix_sents), batch_size):\n",
    "        vsents = wv.vecs[ix_sents[i : i + batch_size]].to(device)\n",
    "        bigram_vsents = bigram_encoder(vsents)\n",
    "        bigram_sentvecs = bigram_vsents.sum(1, keepdim=True)\n",
    "        pos_bigram_ixs = gen_pos_bigram_ixs(ix_sents[i : i + batch_size], device=device)\n",
    "        neg_bigram_ixs = gen_neg_bigram_ixs(ix_sents[i : i + batch_size], device=device)\n",
    "        pos_bigram_vecs = bigram_encoder(wv.vecs[pos_bigram_ixs].to(device))\n",
    "        neg_bigram_vecs = bigram_encoder(wv.vecs[neg_bigram_ixs].to(device))\n",
    "        comparison = cos(bigram_vsents, bigram_sentvecs).min(dim=1, keepdim=True).values\n",
    "        pos_dist = cos(pos_bigram_vecs, bigram_sentvecs)\n",
    "        neg_dist = cos(neg_bigram_vecs, bigram_sentvecs)\n",
    "\n",
    "        result_comparison = torch.cat(\n",
    "            (result_comparison, comparison > neg_dist,), dim=0\n",
    "        )\n",
    "        result_pos_dist = torch.cat((result_pos_dist, pos_dist), dim=0)\n",
    "        result_neg_dist = torch.cat((result_neg_dist, neg_dist), dim=0)\n",
    "    return result_comparison, result_pos_dist, result_neg_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(\n",
    "    bigram_encoder_name, batch_size, outdir=\"paper/img\", seed=0, add_label=True\n",
    "):\n",
    "    bigram_encoder_name_latex = {\n",
    "        \"sign\": r\"f_{\\infty}\",\n",
    "        \"tanh\": r\"f_{1}\",\n",
    "        \"tanh10\": r\"f_{10}\",\n",
    "        \"mult\": r\"f_\\odot\",\n",
    "    }[bigram_encoder_name]\n",
    "    torch.manual_seed(seed)\n",
    "    result_comparison, result_pos_dist, result_neg_dist = analyze_bigram_encoder(\n",
    "        bigram_encoder_name, wv, ix_sents, batch_size, device=device\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Accuracy of {}: {:3.2f}\".format(\n",
    "            bigram_encoder_name_latex, result_comparison.float().mean()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    mpl.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{times}\"\n",
    "    plt.rc(\"text\", usetex=True)\n",
    "    plt.rc(\"font\", family=\"serif\", size=24)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_position([0.22, 0.22, 0.7, 0.7])\n",
    "\n",
    "    ax.set_ylim(top=50000)\n",
    "    ax.set_xlim(left=-1, right=1)\n",
    "    params = {\"alpha\": 0.7, \"bins\": 200}\n",
    "    ax.hist(\n",
    "        result_pos_dist.cpu().numpy(),\n",
    "        label=(r\"$(w, w') \\in B(S)$\" if add_label else \"\"),\n",
    "        **params,\n",
    "        color=\"C2\"\n",
    "    )\n",
    "    ax.hist(\n",
    "        result_neg_dist.cpu().numpy(),\n",
    "        label=(r\"$(w, w') \\notin B(S)$\" if add_label else \"\"),\n",
    "        **params,\n",
    "        color=\"C3\"\n",
    "    )\n",
    "    ax.set(\n",
    "        xlabel=r\"$\\cos(\" + bigram_encoder_name_latex + \"(w, w'), \\mathbf{S^2})$\",\n",
    "        ylabel=r\"\\# sentences\",\n",
    "    )\n",
    "    ax.legend()\n",
    "    plt.savefig(Path(outdir) / \"bigram_encoder_{}.pdf\".format(bigram_encoder_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"mult\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"tanh\", 1000, add_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"tanh10\", 1000, add_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"sign\", 1000, add_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ``gen_pos_examples`` and ``gen_neg_examples``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether all possible positive and negative examples are generated correctly and their counts are distributed evely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_example_generators():\n",
    "    n_rows = 1000000\n",
    "    ix_sents = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 6, 7, 8]]).repeat(n_rows // 2, 1)\n",
    "    sent_lengths = ix_sents.sign().sum(dim=1)\n",
    "\n",
    "    pos_bigram_ixs = gen_pos_bigram_ixs(ix_sents)\n",
    "    neg_bigram_ixs = gen_neg_bigram_ixs(ix_sents)\n",
    "\n",
    "    counter_pos_examples = Counter(tuple(pair) for pair in pos_bigram_ixs.numpy())\n",
    "    counter_neg_examples = Counter(tuple(pair) for pair in neg_bigram_ixs.numpy())\n",
    "\n",
    "    pos_examples_1 = {(1, 2), (2, 3)}\n",
    "    neg_examples_1 = set(product([1, 2, 3], repeat=2)) - pos_examples_1\n",
    "    pos_examples_2 = {(4, 5), (5, 6), (6, 7), (7, 8)}\n",
    "    neg_examples_2 = set(product([4, 5, 6, 7, 8], repeat=2)) - pos_examples_2\n",
    "\n",
    "    total_counts_pos_examples_1 = sum(\n",
    "        [counter_pos_examples[pair] for pair in pos_examples_1]\n",
    "    )\n",
    "    total_counts_neg_examples_1 = sum(\n",
    "        [counter_neg_examples[pair] for pair in neg_examples_1]\n",
    "    )\n",
    "    total_counts_pos_examples_2 = sum(\n",
    "        [counter_pos_examples[pair] for pair in pos_examples_2]\n",
    "    )\n",
    "    total_counts_neg_examples_2 = sum(\n",
    "        [counter_neg_examples[pair] for pair in neg_examples_2]\n",
    "    )\n",
    "\n",
    "    total_counts_pos_examples_1 == total_counts_neg_examples_1 == n_rows // 2\n",
    "    total_counts_pos_examples_2 == total_counts_neg_examples_1 == n_rows // 2\n",
    "\n",
    "    assert sum(counter_pos_examples.values()) == n_rows\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_pos_examples[pair] / total_counts_pos_examples_1\n",
    "                for pair in pos_examples_1\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_pos_examples[pair] / total_counts_neg_examples_1\n",
    "                for pair in neg_examples_1\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_pos_examples[pair] / total_counts_pos_examples_2\n",
    "                for pair in pos_examples_2\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_neg_examples[pair] / total_counts_neg_examples_2\n",
    "                for pair in pos_examples_2\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example_generators()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
