{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import git\n",
    "\n",
    "repo = git.Repo(Path(\".\").absolute(), search_parent_directories=True)\n",
    "ROOT = Path(repo.working_tree_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/scmjhl1/Documents/capturing-word-order\n"
     ]
    }
   ],
   "source": [
    "cd $ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pos_bigram_ixs(ix_sents):\n",
    "    batch_size = ix_sents.shape[0]\n",
    "    sent_lengths = ix_sents.sign().sum(dim=1)\n",
    "    ixs = (\n",
    "        (torch.rand((batch_size,)) * (sent_lengths.float() - 1))\n",
    "        .floor()\n",
    "        .long()\n",
    "        .view(-1, 1)\n",
    "    )\n",
    "    return ix_sents[\n",
    "        torch.arange(batch_size).view(-1, 1), torch.cat((ixs, (ixs + 1)), dim=1),\n",
    "    ]\n",
    "\n",
    "\n",
    "def gen_neg_bigram_ixs(ix_sents):\n",
    "    batch_size, chunk_size = ix_sents.shape\n",
    "    sent_lengths = ix_sents.sign().sum(dim=1)\n",
    "    # ``distr_ixs1`` determines from where to sample the first index. \n",
    "    # All indices but the last one allows for combining with ``sent_lengths`` - 1 \n",
    "    # different second indices.\n",
    "    distr_ixs1 = (sent_lengths.view(-1, 1) - 1) * ix_sents.sign()\n",
    "    # The last index allows for combining with ``sent_lengths`` different second indices\n",
    "    distr_ixs1[torch.arange(batch_size), sent_lengths - 1] = sent_lengths\n",
    "    ixs1 = Categorical(distr_ixs1.float()).sample().view(-1, 1)\n",
    "    # ``distr_ixs2`` determines from where to sample the second index.\n",
    "    # The boundary case is resolved by initializing  ``distr_ixs2`` with\n",
    "    # one extra column and then removing it later.\n",
    "    distr_ixs2 = torch.zeros((batch_size, chunk_size + 1))\n",
    "    distr_ixs2[:, :-1] = ix_sents.sign()\n",
    "    # The indices that lead to positive bigrams are avoided by setting\n",
    "    # the corresponding value in ``distribution`` to zero.\n",
    "    distr_ixs2[torch.arange(batch_size).view(-1, 1), ixs1 + 1] = 0\n",
    "    distr_ixs2 = distr_ixs2[:, :-1]\n",
    "    ixs2 = Categorical(distr_ixs2).sample().view(-1, 1)\n",
    "    return ix_sents[\n",
    "        torch.arange(batch_size).view(-1, 1), torch.cat((ixs1, ixs2), dim=1)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from src.misc import WV, BigramEncoder, load_wiki, process_word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_TEXT = ROOT / \"data/raw/crawl-300d-2M.vec\"\n",
    "word2index, word_vecs = process_word_vecs(FAST_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = load_wiki(max_len=25)\n",
    "# Note that the word embeddings are normalize.\n",
    "wv = WV(F.normalize(word_vecs), word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available options are mult, tanh, tanh10, sign and t\n",
    "bigram_encoder = BigramEncoder(\"sign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Bigram Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix_sents, sent_lengths = wv.to_ix_sents(\n",
    "    sentences, return_sent_lengths=True, adjust=True\n",
    ")\n",
    "perm = torch.randperm(len(ix_sents))\n",
    "ix_sents = ix_sents[perm]\n",
    "sent_lengths = sent_lengths[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0.0\n",
    "i = 0\n",
    "batch_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([153347, 230203, 203036, 232644, 229352, 267902, 208096, 186350, 260220,\n",
       "        177537, 181754, 258873,  29503, 229352,  48408,      1,    762,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_sents[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4171)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "vsents = wv.vecs[ix_sents[i : i + batch_size]]\n",
    "bigram_vsents = bigram_encoder(vsents)\n",
    "bigram_sentvecs = bigram_vsents.sum(1, keepdim=True)\n",
    "pos_bigram_ixs = gen_pos_bigram_ixs(ix_sents[i : i + batch_size])\n",
    "neg_bigram_ixs = gen_neg_bigram_ixs(ix_sents[i : i + batch_size])\n",
    "pos_bigram_vecs = bigram_encoder(wv.vecs[pos_bigram_ixs])\n",
    "neg_bigram_vecs = bigram_encoder(wv.vecs[neg_bigram_ixs])\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=2)\n",
    "(\n",
    "    cos(bigram_vsents, bigram_sentvecs).min(dim=1, keepdim=True).values\n",
    "    > cos(neg_bigram_vecs, bigram_sentvecs)\n",
    ").float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8792)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    cos(pos_bigram_vecs, bigram_sentvecs) > cos(neg_bigram_vecs, bigram_sentvecs)\n",
    ").float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ``gen_pos_examples`` and ``gen_neg_examples``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether all possible positive and negative examples are generated correctly and their counts are distributed evely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_example_generators():\n",
    "    n_rows = 1000000\n",
    "    ix_sents = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 6, 7, 8]]).repeat(n_rows // 2, 1)\n",
    "    sent_lengths = ix_sents.sign().sum(dim=1)\n",
    "\n",
    "    pos_bigram_ixs = gen_pos_bigram_ixs(ix_sents)\n",
    "    neg_bigram_ixs = gen_neg_bigram_ixs(ix_sents)\n",
    "\n",
    "    counter_pos_examples = Counter(tuple(pair) for pair in pos_bigram_ixs.numpy())\n",
    "    counter_neg_examples = Counter(tuple(pair) for pair in neg_bigram_ixs.numpy())\n",
    "\n",
    "    pos_examples_1 = {(1, 2), (2, 3)}\n",
    "    neg_examples_1 = set(product([1, 2, 3], repeat=2)) - pos_examples_1\n",
    "    pos_examples_2 = {(4, 5), (5, 6), (6, 7), (7, 8)}\n",
    "    neg_examples_2 = set(product([4, 5, 6, 7, 8], repeat=2)) - pos_examples_2\n",
    "\n",
    "    total_counts_pos_examples_1 = sum(\n",
    "        [counter_pos_examples[pair] for pair in pos_examples_1]\n",
    "    )\n",
    "    total_counts_neg_examples_1 = sum(\n",
    "        [counter_neg_examples[pair] for pair in neg_examples_1]\n",
    "    )\n",
    "    total_counts_pos_examples_2 = sum(\n",
    "        [counter_pos_examples[pair] for pair in pos_examples_2]\n",
    "    )\n",
    "    total_counts_neg_examples_2 = sum(\n",
    "        [counter_neg_examples[pair] for pair in neg_examples_2]\n",
    "    )\n",
    "\n",
    "    total_counts_pos_examples_1 == total_counts_neg_examples_1 == n_rows // 2\n",
    "    total_counts_pos_examples_2 == total_counts_neg_examples_1 == n_rows // 2\n",
    "\n",
    "    assert sum(counter_pos_examples.values()) == n_rows\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_pos_examples[pair] / total_counts_pos_examples_1\n",
    "                for pair in pos_examples_1\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_pos_examples[pair] / total_counts_neg_examples_1\n",
    "                for pair in neg_examples_1\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_pos_examples[pair] / total_counts_pos_examples_2\n",
    "                for pair in pos_examples_2\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_neg_examples[pair] / total_counts_neg_examples_2\n",
    "                for pair in pos_examples_2\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example_generators()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
