{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import git\n",
    "\n",
    "repo = git.Repo(Path(\".\").absolute(), search_parent_directories=True)\n",
    "ROOT = Path(repo.working_tree_dir)\n",
    "SRC = ROOT / \"src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $SRC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from analyze_bigram_encoders import plot_result\n",
    "from misc import WV, BigramEncoder, load_wiki, process_word_vecs\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_TEXT = ROOT / \"data/raw/crawl-300d-2M.vec\"\n",
    "word2index, word_vecs = process_word_vecs(FAST_TEXT)\n",
    "\n",
    "# Note that the word embeddings are normalized.\n",
    "wv = WV(F.normalize(word_vecs), word2index)\n",
    "# wv = WV(word_vecs, word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = load_wiki(max_len=25)\n",
    "\n",
    "ix_sents, sent_lengths = wv.to_ix_sents(\n",
    "    sentences, return_sent_lengths=True, adjust=True\n",
    ")\n",
    "perm = torch.randperm(len(ix_sents))\n",
    "ix_sents = ix_sents[perm]\n",
    "sent_lengths = sent_lengths[perm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Bigram Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"T\", wv, ix_sents, 100, add_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained on 100 sentences.\n",
    "\n",
    "Train accuracy 0.8116 test accuracy? 0.16\n",
    "\n",
    "I am now training 10 times more epochs. Train accuracy 0.8454 test accuracy: 0.29. Yes there is a big difference.\n",
    "\n",
    "0.8563 -> 0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now training on 1000000 sentences\n",
    "\n",
    "0.8604 -> 0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"mult\", wv, ix_sents, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"tanh\", wv, ix_sents, 1000, add_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"tanh10\", wv, ix_sents, 1000, add_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(\"sign\", wv, ix_sents, 1000, add_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ``gen_pos_examples`` and ``gen_neg_examples``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether all possible positive and negative examples are generated correctly and their counts are distributed evely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_example_generators():\n",
    "    n_rows = 1000000\n",
    "    ix_sents = torch.tensor([[1, 2, 3, 0, 0], [4, 5, 6, 7, 8]]).repeat(n_rows // 2, 1)\n",
    "    sent_lengths = ix_sents.sign().sum(dim=1)\n",
    "\n",
    "    pos_bigram_ixs = gen_pos_bigram_ixs(ix_sents)\n",
    "    neg_bigram_ixs = gen_neg_bigram_ixs(ix_sents)\n",
    "\n",
    "    counter_pos_examples = Counter(tuple(pair) for pair in pos_bigram_ixs.numpy())\n",
    "    counter_neg_examples = Counter(tuple(pair) for pair in neg_bigram_ixs.numpy())\n",
    "\n",
    "    pos_examples_1 = {(1, 2), (2, 3)}\n",
    "    neg_examples_1 = set(product([1, 2, 3], repeat=2)) - pos_examples_1\n",
    "    pos_examples_2 = {(4, 5), (5, 6), (6, 7), (7, 8)}\n",
    "    neg_examples_2 = set(product([4, 5, 6, 7, 8], repeat=2)) - pos_examples_2\n",
    "\n",
    "    total_counts_pos_examples_1 = sum(\n",
    "        [counter_pos_examples[pair] for pair in pos_examples_1]\n",
    "    )\n",
    "    total_counts_neg_examples_1 = sum(\n",
    "        [counter_neg_examples[pair] for pair in neg_examples_1]\n",
    "    )\n",
    "    total_counts_pos_examples_2 = sum(\n",
    "        [counter_pos_examples[pair] for pair in pos_examples_2]\n",
    "    )\n",
    "    total_counts_neg_examples_2 = sum(\n",
    "        [counter_neg_examples[pair] for pair in neg_examples_2]\n",
    "    )\n",
    "\n",
    "    total_counts_pos_examples_1 == total_counts_neg_examples_1 == n_rows // 2\n",
    "    total_counts_pos_examples_2 == total_counts_neg_examples_1 == n_rows // 2\n",
    "\n",
    "    assert sum(counter_pos_examples.values()) == n_rows\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_pos_examples[pair] / total_counts_pos_examples_1\n",
    "                for pair in pos_examples_1\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_pos_examples[pair] / total_counts_neg_examples_1\n",
    "                for pair in neg_examples_1\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_pos_examples[pair] / total_counts_pos_examples_2\n",
    "                for pair in pos_examples_2\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        np.std(\n",
    "            [\n",
    "                counter_neg_examples[pair] / total_counts_neg_examples_2\n",
    "                for pair in pos_examples_2\n",
    "            ]\n",
    "        )\n",
    "        < 0.01\n",
    "    )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example_generators()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
